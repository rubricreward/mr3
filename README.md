# <img src="./assets/logo.png" alt="mR3 Logo" width="45"/> mR3: Multilingual Rubric-Agnostic Reward Reasoning Models

<p align="left">
  <a href="https://huggingface.co/collections/rubricreward/mr3-models-68d970750dd65f186525788d">
    <img src="https://img.shields.io/badge/HF-mR3%20Models-yellow.svg?logo=huggingface&logoColor=white" alt="HF Models"/>
  </a>
  <a href="https://huggingface.co/collections/rubricreward/mr3-datasets-68d9706d9c388e2be8be4b4f">
    <img src="https://img.shields.io/badge/HF-mR3%20Datasets-orange.svg?logo=huggingface&logoColor=white" alt="Training Datasets"/>
  </a>
  <a href="https://huggingface.co/collections/rubricreward/mr3-benchmark-datasets-68d96fb7ec21115e935e0d79">
    <img src="https://img.shields.io/badge/HF-mR3%20Benchmark%20Datasets-orange.svg?logo=huggingface&logoColor=white" alt="Benchmark Datasets"/>
  </a>
  <a href="https://arxiv.org/abs/2510.01146">
    <img src="https://img.shields.io/badge/arXiv-b31b1b.svg?style=flat&logo=arxiv&logoColor=white" alt="arXiv"/>
  </a>
  <a href="https://rubricreward.github.io">
    <img src="https://img.shields.io/badge/ğŸŒ-Website-blue.svg" alt="Website"/>
  </a>
  <a href="https://github.com/rubricreward/mr3/blob/main/LICENSE">
    <img src="https://img.shields.io/github/license/rubricreward/mr3?color=blue" alt="License"/>
  </a>
</p>

<p align="center">
  <img src="./assets/diagram.png" alt="mR3 Diagram" width="90%"/>
</p>

**TL;DR**: **mR3** is the rubric-agnostic reward reasoning (R3) model that is expanded to **72 languages**.
<!-- TODO -->
## ğŸ“¦ Contents

+ [ğŸ¤” Why mR3?](#-why-mr3)
+ [âš™ï¸ Setup Instructions](#%EF%B8%8F-setup-instruction)
+ [ğŸš€ Using Our Model (Inference & Deployment)](#-using-our-model-inference--deployment)
+ [ğŸ§© Using Our Codebase](#-using-our-codebase)
+ [ğŸ“š Citation](#-citation)

## ğŸ¤” Why mR3?

<p align="center">
  <img src="./assets/table1.png" alt="Table 1: mR3 Language Coverage, Model Sizes, Tasks, and Rubric Coverage" width="80%"/>
</p>

**The table above** compares mR3 to existing reward models across key dimensions:
- ğŸŒ **Language Diversity**: mR3 is trained on 72 languages, making mR3 as a reward model with the broadest language coverage to date.
- ğŸ§  **Non-English Reasoning**: mR3 supports non-English reasoning via language forcing, demonstrating performance that even exceeds the English reasoning capacity of the base model, while being interpretable to non-English speakers.
- ğŸ“Š **Task Diversity**: mR3 supports point-wise, pairwise, and binary classification tasksâ€”covering instruction following, reasoning, and factuality.
- ğŸ§© **Rubric Agnosticism**: Unlike many models, mR3 does not rely on fixed evaluation rubrics. Instead, it generalizes across rubricsâ€”making it easily customizable for new use cases.
- ğŸ”“ **Accessibility**: mR3 is open and accessible, making it suitable for lightweight deployment, reproducibility, and extensibility.

### mR3 Dataset Construction

<p align="center">
  <img src="./assets/dataset_train_sample.png" alt="mR3 dataset construction that is aligned across different multilingual settings to highlight the trade-offs between using English and the input language for the prompts and reasoning traces." width="80%"/>
</p>

The final mR3 training dataset totals **100,000 high-quality samples** and was derived from an initial pool of **4M samples across 125 languages.** The curation process involved multiple steps, as outlined below:

#### 1. Initial Data Filtering and Distillation

As shown in **Step 1** of the accompanying figure, the process began with knowledge distillation from the `gpt-oss-120b` model. We employed various strategies to generate reasoning traces (where "target language" refers to the input language):

a. **English Prompt + Target Reasoning**: Using English instructions/rubrics with target-language reasoning.

b. **Target Prompt + English Reasoning**: Using target-language instructions/rubrics with English reasoning.

c. **Target Prompt + Target Reasoning**: Using target-language instructions/rubrics with target-language reasoning.

Crucially, correctness was ensured across all three distillation strategies.

#### 2. Final Curation and Language Alignment

As shown in Step 2 of the accompanying figure, "easy" samples (those where the model could reason correctly without difficulty) were discarded to focus the training on complex examples.

The resulting mR3 dataset of **100,000 samples covers 72 languages.** This dataset is intentionally aligned across different prompt and reasoning language settings for our study. Additionally, we included a scenario:

d. **Target Prompt + Target Reasoning (Translated from English)** for comparison against the language-forcing approach.

For documentation purposes, these are the 72 languages used for training:

<details>

<summary>Click to reveal list of all 72 languages</summary>


```
- aa
- af
- ar
- as
- az
- be
- bg
- bn
- bs
- ca
- cs
- da
- de
- el
- en
- es
- et
- eu
- fa
- fi
- fr
- ha
- he
- hi
- hr
- hu
- hy
- id
- ie
- it
- iw
- ja
- ka
- kk
- ko
- ku
- la
- lt
- lv
- mk
- ms
- my
- nl
- nn
- no
- oc
- pl
- pt
- ro
- ru
- rw
- sa
- sco
- si
- sk
- sl
- sr
- sv
- sw
- ta
- th
- tl
- tlh
- tr
- tt
- uk
- vi
- vo
- war
- xh
- zh
- zh-Hant
```

</details>


<p align="center">
  <img src="./assets/result_preview.png" alt="mR3 Result Breakdown by Multilingual Training Strategy" width="80%"/>
</p>

The figure illustrates the performance change in base models following mR3 training across different multilingual environments. Interestingly, mR3 not only improved general performance but also successfully recovered non-English reasoning, **including surpassing the base model's original English reasoning capacity.** Consequently, mR3 offers improved interpretability for non-English speakers.

## âš™ï¸ Setup Instruction

Python 3.12.11 or higher are recommended. It is recommended to install using `uv` for the following setup.

To install core dependencies for inference and training with `LLaMA-Factory`:
1. Run `pip install -e .` as it will automatically setup everything. Details of dependencies are in `setup.py`.
2. (Optional to reproduce training) Checkout to `src/training/LLaMA-Factory` and do `pip install -e ".[torch,metrics]"`

To run inference with gpt-oss with vLLM (for reproducing baselines), you can run

```bash
uv pip install --pre vllm==0.10.1+gptoss \
    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \
    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
    --index-strategy unsafe-best-match
```

In order to obtain the dataset for RTP-LX, you can unzip `data/rtp_lx.zip` by using the same password as in [RTP-LX official repo](https://github.com/microsoft/RTP-LX).

## ğŸš€ Using Our Model (Inference & Deployment)

You can use our mR3 models directly from our [ğŸ¤— mR3 Models Collection](https://huggingface.co/collections/rubricreward/mr3-models-68d970750dd65f186525788d).

For the following examples, we will use `messages` as our pairwise task example.

<details>

<summary>Click to reveal the example prompt</summary>

```python
system_prompt = """# ì§€ì‹œ

ê·€í•˜ëŠ” ê³µì •í•œ ì‹¬ì‚¬ìœ„ì›ìœ¼ë¡œì„œ, ì£¼ì–´ì§„ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ë‘ ê°œì˜ ë³´ì¡° ë‹µë³€ ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë‚˜ì€ì§€ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ê¸°ì¤€ì˜ ìˆœì„œì— ë”°ë¼ ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”

# í‰ê°€ê¸°ì¤€
Assistant B: Assistant BëŠ” ì „ë°˜ì ìœ¼ë¡œ ë” ë‚˜ì€ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤. íŒë‹¨ ê¸°ì¤€ì€ ë‹¤ìŒ ìˆœì„œëŒ€ë¡œ ì…ë‹ˆë‹¤ â€” ì•ˆì „ì„± ë° ì ì ˆì„±, ìœ ìš©ì„±, ê´€ë ¨ì„±, ê°„ê²°ì„±, ì •ì¤‘í•¨, ê·¸ë¦¬ê³  í¬ê´„ì„±ì…ë‹ˆë‹¤.
Assistant A: Assistant AëŠ” ì „ë°˜ì ìœ¼ë¡œ ë” ë‚˜ì€ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤. íŒë‹¨ ê¸°ì¤€ì€ ë‹¤ìŒ ìˆœì„œëŒ€ë¡œ ì…ë‹ˆë‹¤ â€” ì•ˆì „ì„± ë° ì ì ˆì„±, ìœ ìš©ì„±, ê´€ë ¨ì„±, ê°„ê²°ì„±, ì •ì¤‘í•¨, ê·¸ë¦¬ê³  í¬ê´„ì„±ì…ë‹ˆë‹¤.

# ì‘ë‹µ í˜•ì‹

{'type': 'object', 'properties': {'explanation': {'type': 'string', 'description': 'í‰ê°€ ìš”ì†Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Assistant Aì™€ Assistant Bë¥¼ ë¹„êµí•œ ê°„ë‹¨í•œ ì´ìœ  ì„¤ëª…ì…ë‹ˆë‹¤.'}, 'score': {'type': 'string', 'description': "íŒì • ê²°ê³¼: 'Assistant A' ë˜ëŠ” 'Assistant B' ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.", 'enum': ['Assistant A', 'Assistant B']}}, 'required': ['explanation', 'score']}
"""

user_prompt = """# ì…ë ¥(ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸)
ì²˜í˜•ì´ë€ ë¬´ì—‡ì¸ê°€?

# Assistant A
ì²˜í˜•"ì— ëŒ€í•œ ì •ì˜ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§¥ë½ì— ë”°ë¼ ì˜ë¯¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë²•ì  ì²˜ë²Œì´ë“  í­ë ¥ í–‰ìœ„ì´ë“  ì–´ë–¤ í˜•íƒœì˜ ì²˜í˜•ì´ë“  ê°ˆë“±ì„ í•´ê²°í•˜ê±°ë‚˜ ì‚¬íšŒ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì í•©í•œ í•´ê²°ì±…ì´ ì•„ë‹ˆë¼ëŠ” ì ì„ ê°•ì¡°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ëª¨ë“  ì‚¬ëŒì€ ìƒëª…, ì¡´ì—„ì„±, ì¡´ì¤‘ì„ ë°›ì„ ê¶Œë¦¬ê°€ ìˆìœ¼ë©°, ì°¨ì´ì™€ ê°ˆë“±ì„ í•´ê²°í•˜ëŠ” í‰í™”ë¡­ê³  ê±´ì„¤ì ì¸ ë°©ë²•ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ìš°ë ¤ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë¬¸ì˜í•˜ì„¸ìš”. ë„ì›€ì´ ë˜ê³  ì•ˆì „í•œ ë‹µë³€ì„ ì œê³µí•˜ê¸° ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤.

# Assistant B
ì‚¬í˜• ì§‘í–‰ì€ ë²•ì›ì´ í˜•ëŸ‰, ì¼ë°˜ì ìœ¼ë¡œ í˜•ì‚¬ ìœ ì£„ íŒê²°ì— ëŒ€í•œ ì§•ì—­í˜•ì„ ì§‘í–‰í•˜ë„ë¡ ëª…ë ¹í•˜ëŠ” ë²•ì  ì ˆì°¨ì…ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ê´€í• ê¶Œì˜ ë²•ë¥ ì— ë”°ë¼ ì¹˜ëª…ì  ì£¼ì‚¬, ê°ì „ì‚¬, êµìˆ˜í˜• ë˜ëŠ” ë‹¤ë¥¸ ì‚¬í˜• ë°©ë²•ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬í˜•ì´ ì•„ë‹Œ ê²½ìš°, ì‚¬í˜• ì§‘í–‰ì€ ì§•ì—­í˜•ì„ ì§‘í–‰í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, ì—¬ê¸°ì—ëŠ” êµ¬ê¸ˆ, ë³´í˜¸ê´€ì°° ë˜ëŠ” ê¸°íƒ€ ì²˜ë²Œì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ë‹¹ì‹ ì˜ ì‘ë‹µ
"""

# prepare the model input
messages = [
    {'role': 'system', 'content': system_prompt},
    {'role': 'user', 'content': user_prompt}
]
```
</details>

### ğŸ§  Using `transformers`

#### (1) English Thinking Models

Below is an example of using our `mR3-Qwen3-14B` models when using a non-English prompt and an English thinking using ğŸ¤— `transformers`:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "rubricreward/mR3-Qwen3-14B-tgt-prompt-en-thinking"

# Load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384,
    temperature=0.6, top_p=0.95, min_p=0, top_k=20
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# Parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print(content)
```

#### (2) Non-English Thinking Models

Below is an example of using our `mR3-Qwen3-14B` models when using a non-English prompt and a non-English thinking using language forcing and ğŸ¤— `transformers`:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "rubricreward/mR3-Qwen3-14B-tgt-prompt-tgt-thinking"

# Load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
)

### Key difference: Language forcing in Korean ###
text += "ì•Œê² ìŠµë‹ˆë‹¤. ì €ëŠ” ì œê³µëœ ëª¨ë“  ì •ë³´ë¥¼ ì‹ ì¤‘í•˜ê²Œ ê²€í† í•˜ê³  ì£¼ì–´ì§„ í‰ê°€ ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•œ ë’¤, ìš”ì²­ëœ í˜•ì‹ì— ë§ì¶° ì œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ìƒê°í•˜ë©° ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤."

model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Conduct text completion
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=16384,
    temperature=0.6, top_p=0.95, min_p=0, top_k=20
)
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# Parsing thinking content
try:
    # rindex finding 151668 (</think>)
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

print(content)
```

#### âš¡ Using `vLLM`

Alternatively, you may also use `vLLM` for faster inference (here, showing an example of forcing thinking in Korean):

```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

model_path = "rubricreward/mR3-Qwen3-14B-tgt-prompt-tgt-thinking"
tokenizer = AutoTokenizer.from_pretrained(model_path)
sampling_params = SamplingParams(temperature=0.6, top_p=0.95, max_tokens=16384, min_p=0, top_k=20)

llm = LLM(
  model=model_path,
  dtype="bfloat16",
  max_model_len=32768,
)

list_text = tokenizer.apply_chat_template(
  messages,
  tokenize=False,
  add_generation_prompt=True,
  enable_thinking=True # Switch between thinking and non-thinking modes. 
)

for index in range(len(list_text)):
    ### Key difference: Language forcing in Korean ###
    list_text[index] += "ì•Œê² ìŠµë‹ˆë‹¤. ì €ëŠ” ì œê³µëœ ëª¨ë“  ì •ë³´ë¥¼ ì‹ ì¤‘í•˜ê²Œ ê²€í† í•˜ê³  ì£¼ì–´ì§„ í‰ê°€ ê¸°ì¤€ì— ë”°ë¼ í‰ê°€í•œ ë’¤, ìš”ì²­ëœ í˜•ì‹ì— ë§ì¶° ì œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê²Œ ìƒê°í•˜ë©° ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤."

outputs = llm.generate(list_text, sampling_params)
print(outputs[0].output.text)
```

## ğŸ§© Using Our Codebase

This codebase is primarily intended for reproducing our experiments. It consists of the following components:
- `src/training`: Training configuration using LLaMA Factory
- `src/evaluation`: Evaluation pipeline for the mR3 benchmark
- `scripts/create_config.py`: Evaluation configurations used in our experiments. It automatically creates JSON file to create the config
- `data/mr3_rubrics`: Automatically generated rubrics along with the translations for each dataset using GPT-4.1

## ğŸ“š Citation

If you found our work helpful, please cite our work using the following citation!

```bibtex
@article{anugraha2025mr3,
  title={mR3: Multilingual Rubric-Agnostic Reward Reasoning Models},
  author={Anugraha, David and Hung, Shou-Yi and Tang, Zilu and Lee, Annie En-Shiun and Wijaya, Derry and Winata, Genta Indra},
  journal={arXiv preprint arXiv:2510.01146},
  year={2025}
}
```

If you have any questions, you can open a [GitHub Issue](https://github.com/rubricreward/mr3/issues)!
